---
title: "Assignment 4 - Nikola Metes"
output: html_notebook
---

```{r,include=FALSE}
install.packages("tidyverse")
install.packages("car")
install.packages("pROC")
```
```{r}
library(tidyverse)
library(car)
library(pROC)
```
```{r}
# load the data set
GermanCredit <- read_csv(file.path("Data", "GermanCredit_modified_SP19_001.csv")) %>% 
  mutate(good=if_else(Class=="Good",1,0)) %>% 
  select(-Class)
```

```{r}
# Split data into train and test
 set.seed(737900)

# set an index to split the data set  
# 
# Create the train data frame
s <- sample(nrow(GermanCredit), replace=FALSE, size = .8*nrow(GermanCredit)) 
trainDF <- GermanCredit[s,]
# 
# Create the test data frame  
testDF <- GermanCredit[-s,]
# 
```

# 2. Fit a logistic regression model in R

Fit a logistic regression model to predict the Class variable using all of the predictors in trainDF and assign the fitted model to the object logit.fit1.

MISSING CODE

```{r}
str(GermanCredit)
trainDF %>% select(Class) %>% table()
```

```{r}
logit.fit1 <- glm(good~.,family=binomial,data=trainDF)
Anova(logit.fit1)
```

# a. Comment on the model’s Residual deviance as compared to both the degrees of freedom and the Null deviance. Is this a “good” model for the prediction of Class based on these statistics alone?
Residual deviance is less than the Null deviance. Siginificant decrease in the null deviance. The p value is 5.606281e-11. The model is yes, since the p value is close to zero.

```{r}
summary(logit.fit1)
```
```{r}
253.67-159.08
```
```{r}
pchisq(94.59,df=199-177,lower.tail = FALSE)
```

# b. Which of the coefficients are most significant?
Duration, Checking, Employment.Duration
Duration - for every increase in one unit of Duration, the odds of Good decrease proportionally by a multiplying the number by 0.9418022. 
Checking - everything is in reference to Checking 0.to.200 group.
```{r}
exp(-0.05996)
```
```{r}
table(GermanCredit$Checking)
```
coefficients
0.93248, -1.32789, 0.80624 

#c. Interpret, in plain english, the Duration and Amount coefficients. How do they effect our prediction of the Class variable.
The Amount has been removed and has noe effect on the model.

#d. Interpret, in plain english, the Intercept coefficient of this model. Remember that the Intercept in logistic regression is subject to the same interpretation of factor variables as linear regression.
when all the continuous variables are zero and the factor variables are their reference then the predicted log odds (4.496e+00) is the intercept.


# 3.1 Confusion Matrix: Train
```{r}
log.50 <- logit.fit1$fitted.values
log.50[log.50>=0.5] <- 1
log.50[log.50<0.5] <- 0  
```

# Create factor vectors
```{r}
actual <- trainDF$good
predicted <- factor(log.50, levels = c(0,1), labels = c("Bad", "Good"))
```


```{r}
# Print the confusion matrix
table(actual, predicted)   
```
```{r}
round(prop.table(table(actual, predicted),1),2)
```

#a. What is the specificity and sensitivity of this model on the train data set?
seinsitivity - ability to detect an outcome
specificity - ability to detect when there is not an ability to detect and outcome (ie an accident) can't differentiate between if somebody has it or not; how many NO items are we going to guess correctly

#b. Is this a good model at a .5 threshold? HINT: Do you think this institution would rather accurately predict cases of Good credit or cases of Bad credit?
they may want to vary that threshold number

```{r}
plot(roc(trainDF$good, logit.fit1$fitted.values))
```


```{r}
auc(trainDF$good, logit.fit1$fitted.values)
```
## Area under the curve: 0.8372
# a. What does the above output from the ROC curve tell you about this model?
if you can pair two people and person 1 scores higher on the model than person 2, they should both be rated as the same and/or person 1 should be rated as good

if somebody scores higher on the model, they should be good
if it is opposite then it is negative

# b. Does this change your interpretation of this being a good model?
the enlarged area under the ROC curve indicates that the mdodel is good. 0.83 is pretty good



# 4.1 Confusion Matrix: Test
```{r}
log.test <- predict(logit.fit1, newdata = testDF, type = "response")
log.test[log.test>=0.5] <- 1
log.test[log.test<0.5] <- 0

```

# Create factor vectors
```{r}
actual <- testDF$good
predicted <- factor(log.test, levels = c(0,1), labels = c("Bad", "Good"))
```


```{r}
# Print the confusion matrix
table(actual, predicted)   
```

```{r}
round(prop.table(table(actual, predicted),1),2)

```

# a. How well did your model perform against the holdout dataset?
The prediction of Good went down down from .90 to .87
True positives



# 5. Improved Model

```{r}
logit.fit1 <- glm(good~.,family=binomial,data=trainDF)
Anova(logit.fit1)
```


```{r}
logit.fit1 <- trainDF %>% 
  select(-ResidenceDuration) %>% 
  glm(good~.,family=binomial,data=.)
Anova(logit.fit1)
```
```{r}
logit.fit1 <- trainDF %>% 
  select(-ResidenceDuration,-Age) %>% 
  glm(good~.,family=binomial,data=.)
Anova(logit.fit1)
```
```{r}
logit.fit1 <- testDF %>% 
  select (-ResidenceDuration, -Age, -Job.Type) %>% 
  glm(good~.,family=binomial,data=.)
Anova(logit.fit1)
  
```
```{r}
logit.fit1 <- testDF %>% 
  select (-ResidenceDuration, - Age, -Job.Type, -NumberPeopleMaintenance) %>% 
  glm(good~.,family=binomial,data=.)
Anova(logit.fit1)
```

```{r}
logit.fit1 <- testDF %>% 
  select (-ResidenceDuration, -Age, -Job.Type, -NumberPeopleMaintenance, -Telephone) %>% 
  glm(good~.,family=binomial,data=.)
Anova(logit.fit1)
```

```{r}
logit.fit1 <- testDF %>% 
  select (-ResidenceDuration, -Age, -Job.Type, -NumberPeopleMaintenance, -Telephone, -NumberExistingCredits) %>% 
  glm(good~.,family=binomial,data=.)
Anova(logit.fit1)
```
```{r}
logit.fit1 <- testDF %>% 
  select (-ResidenceDuration, -Age, -Job.Type, -NumberPeopleMaintenance, -Telephone, -NumberExistingCredits, -Other.Debtors) %>% 
  glm(good~.,family=binomial,data=.)
Anova(logit.fit1)
```
```{r}
logit.fit1 <- testDF %>% 
  select (-ResidenceDuration, -Age, -Job.Type, -NumberPeopleMaintenance, -Telephone, -NumberExistingCredits, -Other.Debtors, -Property) %>% 
  glm(good~.,family=binomial,data=.)
Anova(logit.fit1)
```
```{r}
logit.fit1 <- testDF %>% 
  select (-ResidenceDuration, -Age, -Job.Type, -NumberPeopleMaintenance, -Telephone, -NumberExistingCredits, -Other.Debtors, -Property, -Credit.History) %>% 
  glm(good~.,family=binomial,data=.)
Anova(logit.fit1)
```
```{r}
logit.fit1 <- testDF %>% 
  select (-ResidenceDuration, -Age, -Job.Type, -NumberPeopleMaintenance, -Telephone, -NumberExistingCredits, -Other.Debtors, -Property, -Credit.History, -Savings) %>% 
  glm(good~.,family=binomial,data=.)
Anova(logit.fit1)
```
```{r}
logit.fit1 <- testDF %>% 
  select (-ResidenceDuration, -Age, -Job.Type, -NumberPeopleMaintenance, -Telephone, -NumberExistingCredits, -Other.Debtors, -Property, -Credit.History, -Savings, -Amount) %>% 
  glm(good~.,family=binomial,data=.)
Anova(logit.fit1)
```
```{r}
logit.fit1 <- testDF %>% 
  select (-ResidenceDuration, -Age, -Job.Type, -NumberPeopleMaintenance, -Telephone, -NumberExistingCredits, -Other.Debtors, -Property, -Credit.History, -Savings, -Amount, -Personal.Status) %>% 
  glm(good~.,family=binomial,data=.)
Anova(logit.fit1)
```
